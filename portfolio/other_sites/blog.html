<!DOCTYPE html>
<html lang="fi">

<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Junior Dev Blog">
    <meta name="author" content="al-lu">
    <title>Blog</title>

    <link rel="shortcut icon" href="../img/logo.png">
    <link rel="stylesheet" href="../css/blog.css">
</head>

<body class="blog">

    <header id="header-top" class="wrapper-wraps">
        <a href="../index.html" class="logo"><img src="../img/logo.png" style="max-width: 30%;"></a>
        <nav class="nav">
            <ul>
                <li><a class="home" href="../index.html">Home</a></li>
            </ul>
        </nav>
    </header>

    <section class="banner-intro">
        <h1>
            Dev blog
        </h1>
        <p class="subheading">
            How my project works and what I learned by doing them.
        </p>
    </section>

    <section class="wrapper layout">
        <div class="main-col">

            <article class="blog-entry">
                <header>
                    <h1 class="entry-title">
                        <a rel="bookmark" id="bookmark-tess" href="">Tesseract OCR</a>
                    </h1>
                    <div class="author vcard">
                        by <b class="fn n">al-lu</b>
                    </div>
                    <time class="published updated" datetime="2022-01-05T00:00:00+00:00" pubdate>
                        Wednesday, 5 January 2022
                    </time>
                </header>
                <div class="project-gif">
                    <img src="../demos/ocr_demo.gif">
                </div>
                <div class="entry-content">
                    <h2 id="sub-heading">Project description</h2>
                    <p>I wanted to see how Tesseract works and how it could be used as a real-time OCR "optical
                        character recognition" engine. The way this
                        works
                        is that firstly if I want to achieve anykind of real-time detections I have to grab my second
                        monitor and feed it to Tesseract. By extracting the video feed to individual frames and then
                        using openCV and Numpy I can use Tesseract on frames that I never have to save in JPG or other
                        image formats meaning that the image data always stays as a numpy array. This reduces the
                        time required to run the loop resulting in more frames per second being fed to the OCR engine.
                        The reason why the demo looks a bit laggy is that I'm getting the full content from my monitor
                        resulting in larger image data and more ROIs "regions of intrest" to detect and recognise. Other
                        thing I notised when running the script was that Tesseract works better on larger text. Resizing
                        images and other nessesary image manipulations could be done with openCV. Also the script could
                        be run without any visualizations and the text could be extracted straight from the detections
                        array, I just find it easier to visualize the detections for the demos.
                    </p>
                    <ol class="footnotes">
                        <li id="footnote-1">
                            Link to Tesseract: <a href="https://tesseract-ocr.github.io/tessdoc/">Tesseract User
                                Manual</a>
                        </li>
                    </ol>


                </div>
            </article>

            <nav class="back">
                <a href="../index.html" class="button">&larr; Back</a>
            </nav>